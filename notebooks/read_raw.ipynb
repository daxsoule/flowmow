{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read_raw\n",
    "\n",
    "This notebook reads the raw FlowMow2 Sentry data into Pandas dataframes and saves them to HDF5 files. This notebook does not do any processing on the data aside from assigning timestamps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import glob\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timestamp extractor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp(line):\n",
    "    timestamp = dt.datetime.strptime(' '.join(line.strip().split(' ')[1:3]), '%Y/%m/%d %H:%M:%S.%f')\n",
    "    epoch = np.float64(timestamp.replace(tzinfo=dt.timezone.utc).timestamp()) # 'epoch' is unix time\n",
    "    return timestamp, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import nav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# list of matlab rnv files\n",
    "filenames = 'ashes_nav.list'\n",
    "\n",
    "# initialize lists\n",
    "timestamp = []\n",
    "epoch = []\n",
    "dive_number = []\n",
    "lat = []\n",
    "lon = []\n",
    "depth = []\n",
    "height = []\n",
    "heading = []\n",
    "pitch = []\n",
    "roll = []\n",
    "\n",
    "# loop through matlab files and extract nav data\n",
    "with open(filenames, 'r') as f:\n",
    "    for filename in f:\n",
    "        nav_mat = sio.loadmat(filename.strip(), squeeze_me=True)\n",
    "        nrows = len(nav_mat['rnv']['t'].take(0))\n",
    "        for i in range(nrows):\n",
    "            timestamp.append(dt.datetime.utcfromtimestamp(nav_mat['rnv']['t'].take(0)[i]))\n",
    "        epoch.extend(nav_mat['rnv']['t'].take(0))        \n",
    "        dive_number.extend(np.ones((nrows,), dtype=np.int64) * np.int64(filename.split('/')[7][-3:]))\n",
    "        lat.extend(nav_mat['rnv']['lat'].take(0))\n",
    "        lon.extend(nav_mat['rnv']['lon'].take(0))\n",
    "        depth.extend(nav_mat['rnv']['pos'].take(0)[:,2])\n",
    "        height.extend(nav_mat['rnv']['alt'].take(0))\n",
    "        heading.extend(nav_mat['rnv']['pos'].take(0)[:,3])\n",
    "        pitch.extend(nav_mat['rnv']['pos'].take(0)[:,4])\n",
    "        roll.extend(nav_mat['rnv']['pos'].take(0)[:,5])\n",
    "\n",
    "# convert to dataframe\n",
    "nav = pd.DataFrame({'timestamp': timestamp, 'epoch': epoch, 'dive_number': dive_number,\n",
    "                    'lat': lat, 'lon': lon, 'depth': depth, 'heading': heading,\n",
    "                    'pitch': pitch, 'roll': roll, 'height': height})\n",
    "\n",
    "# reorder columns\n",
    "nav = nav[['timestamp', 'epoch', 'dive_number', 'lat', 'lon', 'depth', 'height', 'heading', 'pitch', 'roll']]\n",
    "\n",
    "# save to hdf5\n",
    "nav.to_hdf('nav.h5', 'table', append=False, data_columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Paros pressure sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# list of paros DAT files\n",
    "filenames = 'ashes_paros.list'\n",
    "\n",
    "# loop through all files and append data to list\n",
    "paros_list = []\n",
    "i = 0\n",
    "with open(filenames, 'r') as f:\n",
    "    for filename in f:\n",
    "        dive_number = np.int64(filename.split('/')[7][-3:])\n",
    "        with open(filename.strip(), 'r') as g:\n",
    "            for line in g:\n",
    "                if 'RAW' in line.strip()[0:3]:\n",
    "                    if 'P2=' in line.strip():\n",
    "                        if len(line) == 58: # good lines from this instrument are length 58\n",
    "                            timestamp, epoch = get_timestamp(line)\n",
    "                            a = line.strip().split(' ')[3].split(',')[0].split('=')[1]\n",
    "                            b = line.strip().split(' ')[3].split(',')[1]\n",
    "                            paros_list.append([timestamp, epoch, dive_number, np.float64(a), np.float64(b)])\n",
    "                            i = i+1\n",
    "\n",
    "# convert to dataframe (tau and eta are the the pressure and temperature signal periods in microseconds)\n",
    "paros = pd.DataFrame(paros_list, columns=['timestamp', 'epoch', 'dive_number', 'tau', 'eta'])\n",
    "\n",
    "# save to hdf5\n",
    "paros.to_hdf('paros.h5', 'table', append=False, data_columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import stinger GX3-25 microstrain IMU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# list of ustrain_adv DAT files\n",
    "filenames = 'ashes_ustrain_adv.list'\n",
    "\n",
    "# loop through all files and append data to list\n",
    "ustrain_adv_list = []\n",
    "with open(filenames, 'r') as f:\n",
    "    for filename in f:\n",
    "        dive_number = np.int64(filename.split('/')[7][-3:])\n",
    "        with open(filename.strip(), 'r') as g:\n",
    "            for line in g:\n",
    "                if 'MSA3' in line.strip()[0:4]:\n",
    "                    if len(line.split(' ')) == 33: # good lines from this instrument will have 33 fields\n",
    "                        timestamp, epoch = get_timestamp(line)\n",
    "                        ustrain_adv_list.append([timestamp, epoch, dive_number] +\n",
    "                                                list(map(np.float64, line.strip().split(' ')[3:-1])))\n",
    "\n",
    "# convert to dataframe\n",
    "ustrain_adv = pd.DataFrame(ustrain_adv_list, columns=['timestamp','epoch','dive_number','a',\n",
    "                                                      'b','c','d','e','f','g','h','i','j','k',\n",
    "                                                      'l','m','n','o','p','q','r','s','t','u',\n",
    "                                                      'v','w','x','y','z','aa','bb'])\n",
    "\n",
    "# save to hdf5\n",
    "ustrain_adv.to_hdf('ustrain_adv.h5', 'table', append=False, data_columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import chassis GX3-25 microstrain IMU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# list of ustrain_chassis DAT files\n",
    "filenames = 'ashes_ustrain_chassis.list'\n",
    "\n",
    "# loop through all files and append data to list\n",
    "ustrain_chassis_list = []\n",
    "with open(filenames, 'r') as f:\n",
    "    for filename in f:\n",
    "        dive_number = np.int64(filename.split('/')[7][-3:])\n",
    "        with open(filename.strip(), 'r') as g:\n",
    "            for line in g:\n",
    "                if 'MSA3' in line.strip()[0:4]:\n",
    "                    if len(line.split(' ')) == 33: # good lines from this instrument will have 33 fields\n",
    "                        timestamp, epoch = get_timestamp(line)\n",
    "                        ustrain_chassis_list.append([timestamp, epoch, dive_number] +\n",
    "                                                    list(map(np.float64, line.strip().split(' ')[3:-1])))\n",
    "\n",
    "# convert to dataframe\n",
    "ustrain_chassis = pd.DataFrame(ustrain_chassis_list, columns=['timestamp','epoch','dive_number','a',\n",
    "                                                              'b','c','d','e','f','g','h','i','j','k',\n",
    "                                                              'l','m','n','o','p','q','r','s','t','u',\n",
    "                                                              'v','w','x','y','z','aa','bb'])\n",
    "\n",
    "# save to hdf5\n",
    "ustrain_chassis.to_hdf('ustrain_chassis.h5', 'table', append=False, data_columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import SBE3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# list of ustrain_chassis DAT files\n",
    "filenames = 'ashes_sbe3.list'\n",
    "\n",
    "# loop through all files and append data to list\n",
    "sbe3_list = []\n",
    "with open(filenames, 'r') as f:\n",
    "    for filename in f:\n",
    "        dive_number = np.int64(filename.split('/')[7][-3:])\n",
    "        with open(filename.strip(), 'r', encoding='utf-8', errors='ignore') as g:\n",
    "            for line in g:\n",
    "                if 'SBE3' in line.strip()[0:4]:\n",
    "                    if len(line.strip()) == 58: # good lines from this instrument are length 58\n",
    "                        timestamp, epoch = get_timestamp(line)\n",
    "                        counts_0 = np.int64(line.strip().split(' ')[4])\n",
    "                        counts_1 = np.int64(line.strip().split(' ')[6])\n",
    "                        if counts_0 > 500000 and counts_0 < 815000 and counts_1 > 450000 and counts_1 < 770000:\n",
    "                            sbe3_list.append([timestamp, epoch, dive_number, counts_0, counts_1])\n",
    "\n",
    "# convert to dataframe\n",
    "sbe3 = pd.DataFrame(sbe3_list, columns=['timestamp','epoch','dive_number','counts_0','counts_1'])\n",
    "\n",
    "# save to hdf5\n",
    "sbe3.to_hdf('sbe3.h5', 'table', append=False, data_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbe3.epoch.diff().plot(linewidth=0, marker='.', markersize=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
