{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read_raw\n",
    "\n",
    "This notebook reads the raw FlowMow Sentry data into Pandas dataframes and saves them to HDF5 files using HDFStore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import glob\n",
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timestamp extractor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp(line):\n",
    "    timestamp = dt.datetime.strptime(' '.join(line.strip().split(' ')[1:3]), '%Y/%m/%d %H:%M:%S.%f')\n",
    "    epoch = timestamp.replace(tzinfo=dt.timezone.utc).timestamp()\n",
    "    return timestamp, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import SCC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filenames = glob.glob('/home/tjc/research/flowmow/**/*.scc', recursive=True)\n",
    "filenames.sort()\n",
    "scc_list = []\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'SCC' in line.strip()[0:3]:\n",
    "                timestamp, epoch = get_timestamp(line)\n",
    "                scc_list.append([timestamp, epoch] + list(map(np.float64, line.strip().split(' ')[3:])))           \n",
    "scc = pd.DataFrame(scc_list, columns=['timestamp','epoch','lat','lon','depth','pressure','heading',\n",
    "                                      'magx','magy','magz','obs','eh','aux1','aux2','T1','C1',\n",
    "                                      'T2','C2','S1','S2','ss1','depth_d','height','D1','D2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use GMT to get UTM values for Sentry navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tmpfile = dt.datetime.now().strftime(\"utmx_utmy_%Y%m%d%H%M%S%f\")\n",
    "scc.to_csv(tmpfile, sep=',', columns = ['lon', 'lat'], header=False, index=False)\n",
    "cmd = 'cat %s | gmt mapproject -Ju9/1 -R-132/-126/40/48 -F' % tmpfile\n",
    "gmt_output = (subprocess.check_output(cmd, shell=True).decode('utf-8')).split('\\n')\n",
    "os.remove(tmpfile)\n",
    "os.remove('gmt.history')\n",
    "\n",
    "utm_x = []\n",
    "utm_y = []\n",
    "for i in gmt_output:\n",
    "    try:\n",
    "        utm_x.append(np.float64(i.split('\\t')[0]))\n",
    "        utm_y.append(np.float64(i.split('\\t')[1]))\n",
    "    except:\n",
    "        pass\n",
    "scc.insert(4, 'utm_x', utm_x)\n",
    "scc.insert(5, 'utm_y', utm_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save SCC to an HDF5 store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "store = pd.HDFStore('flowmow.h5')\n",
    "store['scc'] = scc\n",
    "store.flush(fsync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Paros pressure sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filenames = glob.glob('/home/tjc/research/flowmow/**/parosKinsey/*.DAT', recursive=True)\n",
    "filenames.sort()\n",
    "paros_list = []\n",
    "i = 0\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'RAW' in line.strip()[0:3]:\n",
    "                if 'P2=' in line.strip():\n",
    "                    timestamp, epoch = get_timestamp(line)\n",
    "                    a = line.strip().split(' ')[3].split(',')[0].split('=')[1]\n",
    "                    b = line.strip().split(' ')[3].split(',')[1]\n",
    "                    paros_list.append([timestamp,epoch,a,b])\n",
    "                    i = i+1\n",
    "paros = pd.DataFrame(paros_list, columns=['timestamp', 'epoch', 'a', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push paros to the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "store['paros'] = paros\n",
    "store.flush(fsync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import stinger GX3-25 microstrain IMU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filenames = glob.glob('/home/tjc/research/flowmow/**/ustrainAdv/*.DAT', recursive=True)\n",
    "filenames.sort()\n",
    "ustrain_adv_list = []\n",
    "i = 0\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'MSA3' in line.strip()[0:4]:\n",
    "                timestamp, epoch = get_timestamp(line)\n",
    "                ustrain_adv_list.append([timestamp, epoch] + list(map(np.float64, line.strip().split(' ')[3:-1])))\n",
    "ustrain_adv = pd.DataFrame(ustrain_adv_list, columns=['timestamp','epoch','a','b','c','d','e','f',\n",
    "                                                      'g','h','i','j','k','l','m','n','o','p','q',\n",
    "                                                      'r','s','t','u','v','w','x','y','z','aa','bb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push ustrain_adv to store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "store['ustrain_adv'] = ustrain_adv\n",
    "store.flush(fsync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import chassis GX3-25 microstrain IMU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filenames = glob.glob('/home/tjc/research/flowmow/**/ustrain-chassis/*.DAT', recursive=True)\n",
    "filenames.sort()\n",
    "ustrain_chassis_list = []\n",
    "i = 0\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'MSA3' in line.strip()[0:4]:\n",
    "                timestamp, epoch = get_timestamp(line)\n",
    "                ustrain_chassis_list.append([timestamp, epoch] + list(map(np.float64, line.strip().split(' ')[3:-1])))\n",
    "ustrain_chassis = pd.DataFrame(ustrain_chassis_list, columns=['timestamp','epoch','a','b','c','d','e','f',\n",
    "                                                      'g','h','i','j','k','l','m','n','o','p','q',\n",
    "                                                      'r','s','t','u','v','w','x','y','z','aa','bb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push ustrain_chassis to store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "store['ustrain_chassis'] = ustrain_chassis\n",
    "store.flush(fsync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import SBE3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get list of data files\n",
    "filenames = glob.glob('/home/tjc/research/flowmow/**/sbe3/*.DAT', recursive=True)\n",
    "filenames.sort()\n",
    "\n",
    "# loop through all files and append data to list\n",
    "sbe3_list = []\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'RAW' in line.strip()[0:3]:\n",
    "                timestamp, epoch = get_timestamp(line)\n",
    "                sbe3_list.append([timestamp, epoch] + list(map(np.uint32, line.strip().split(' ')[4:6])))\n",
    "\n",
    "# convert to dataframe\n",
    "sbe3 = pd.DataFrame(sbe3_list, columns=['timestamp','epoch','a','b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push sbe3 to store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "store['sbe3'] = sbe3\n",
    "store.flush(fsync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Close store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
