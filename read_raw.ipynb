{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read_raw\n",
    "\n",
    "This notebook reads the raw FlowMow2 Sentry data into Pandas dataframes and saves them to HDF5 files. This notebook does not do any processing on the data aside from assigning timestamps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import glob\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timestamp extractor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp(line):\n",
    "    timestamp = dt.datetime.strptime(' '.join(line.strip().split(' ')[1:3]), '%Y/%m/%d %H:%M:%S.%f')\n",
    "    epoch = np.float64(timestamp.replace(tzinfo=dt.timezone.utc).timestamp()) # 'epoch' is unix time\n",
    "    return timestamp, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import nav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# list of matlab rnv files\n",
    "filenames = '/home/tjc/research/flowmow/data/dives/ashes_nav.list'\n",
    "\n",
    "# initialize lists\n",
    "timestamp = []\n",
    "epoch = []\n",
    "dive_number = []\n",
    "lat = []\n",
    "lon = []\n",
    "depth = []\n",
    "height = []\n",
    "heading = []\n",
    "pitch = []\n",
    "roll = []\n",
    "\n",
    "# loop through matlab files and extract nav data\n",
    "with open(filenames, 'r') as f:\n",
    "    for filename in f:\n",
    "        nav_mat = sio.loadmat(filename.strip(), squeeze_me=True)\n",
    "        nrows = len(nav_mat['rnv']['t'].take(0))\n",
    "        for i in range(nrows):\n",
    "            timestamp.append(dt.datetime.utcfromtimestamp(nav_mat['rnv']['t'].take(0)[i]))\n",
    "        epoch.extend(nav_mat['rnv']['t'].take(0))        \n",
    "        dive_number.extend(np.ones((nrows,), dtype=np.int64) * np.int64(filename.split('/')[7][-3:]))\n",
    "        lat.extend(nav_mat['rnv']['lat'].take(0))\n",
    "        lon.extend(nav_mat['rnv']['lon'].take(0))\n",
    "        depth.extend(nav_mat['rnv']['pos'].take(0)[:,2])\n",
    "        height.extend(nav_mat['rnv']['alt'].take(0))\n",
    "        heading.extend(nav_mat['rnv']['pos'].take(0)[:,3])\n",
    "        pitch.extend(nav_mat['rnv']['pos'].take(0)[:,4])\n",
    "        roll.extend(nav_mat['rnv']['pos'].take(0)[:,5])\n",
    "\n",
    "# convert to dataframe\n",
    "nav = pd.DataFrame({'timestamp': timestamp, 'epoch': epoch, 'dive_number': dive_number,\n",
    "                    'lat': lat, 'lon': lon, 'depth': depth, 'heading': heading,\n",
    "                    'pitch': pitch, 'roll': roll, 'height': height})\n",
    "\n",
    "# reorder columns\n",
    "nav = nav[['timestamp', 'epoch', 'dive_number', 'lat', 'lon', 'depth', 'height', 'heading', 'pitch', 'roll']]\n",
    "\n",
    "# save to hdf5\n",
    "nav.to_hdf('nav.h5', 'table', append=False, data_columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Paros pressure sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# list of paros DAT files\n",
    "filenames = '/home/tjc/research/flowmow/data/dives/ashes_paros.list'\n",
    "\n",
    "# loop through all files and append data to list\n",
    "paros_list = []\n",
    "i = 0\n",
    "with open(filenames, 'r') as f:\n",
    "    for filename in f:\n",
    "        dive_number = np.int64(filename.split('/')[7][-3:])\n",
    "        with open(filename.strip(), 'r') as g:\n",
    "            for line in g:\n",
    "                if 'RAW' in line.strip()[0:3]:\n",
    "                    if 'P2=' in line.strip():\n",
    "                        if len(line) == 58: # good lines from this instrument are length 58\n",
    "                            timestamp, epoch = get_timestamp(line)\n",
    "                            a = line.strip().split(' ')[3].split(',')[0].split('=')[1]\n",
    "                            b = line.strip().split(' ')[3].split(',')[1]\n",
    "                            paros_list.append([timestamp, epoch, dive_number, np.float64(a), np.float64(b)])\n",
    "                            i = i+1\n",
    "\n",
    "# convert to dataframe (tau and eta are the the pressure and temperature signal periods in microseconds)\n",
    "paros = pd.DataFrame(paros_list, columns=['timestamp', 'epoch', 'dive_number', 'tau', 'eta'])\n",
    "\n",
    "# save to hdf5\n",
    "paros.to_hdf('paros.h5', 'table', append=False, data_columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import stinger GX3-25 microstrain IMU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# list of ustrain_adv DAT files\n",
    "filenames = '/home/tjc/research/flowmow/data/dives/ashes_ustrain_adv.list'\n",
    "\n",
    "# loop through all files and append data to list\n",
    "ustrain_adv_list = []\n",
    "\n",
    "with open(filenames, 'r') as f:\n",
    "    for filename in f:\n",
    "        dive_number = np.int64(filename.split('/')[7][-3:])\n",
    "        with open(filename.strip(), 'r') as g:\n",
    "            for line in g:\n",
    "                if 'MSA3' in line.strip()[0:4]:\n",
    "                    if len(line.split(' ')) == 33: # good lines from this instrument will have 33 fields\n",
    "                        timestamp, epoch = get_timestamp(line)\n",
    "                        ustrain_adv_list.append([timestamp, epoch, dive_number] +\n",
    "                                                list(map(np.float64, line.strip().split(' ')[3:-1])))\n",
    "\n",
    "# convert to dataframe\n",
    "ustrain_adv = pd.DataFrame(ustrain_adv_list, columns=['timestamp','epoch','dive_number','a',\n",
    "                                                      'b','c','d','e','f','g','h','i','j','k',\n",
    "                                                      'l','m','n','o','p','q','r','s','t','u',\n",
    "                                                      'v','w','x','y','z','aa','bb'])\n",
    "\n",
    "# save to hdf5\n",
    "ustrain_adv.to_hdf('ustrain_adv.h5', 'table', append=False, data_columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import chassis GX3-25 microstrain IMU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get list of data files\n",
    "filenames = glob.glob('/home/tjc/research/flowmow/**/ustrain-chassis/*.DAT', recursive=True)\n",
    "filenames.sort()\n",
    "\n",
    "# loop through all files and append data to list\n",
    "ustrain_chassis_list = []\n",
    "for filename in filenames:\n",
    "    dive_number = np.int64(filename.split('/')[5][-3:])\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'MSA3' in line.strip()[0:4]:\n",
    "                if len(line.split(' ')) == 33: # good lines from this instrument will have 33 fields\n",
    "                    timestamp, epoch = get_timestamp(line)\n",
    "                    ustrain_chassis_list.append([timestamp, epoch, dive_number] +\n",
    "                                                list(map(np.float64, line.strip().split(' ')[3:-1])))\n",
    "\n",
    "# convert to dataframe\n",
    "ustrain_chassis = pd.DataFrame(ustrain_chassis_list, columns=['timestamp','epoch','dive_number','a',\n",
    "                                                              'b','c','d','e','f','g','h','i','j','k',\n",
    "                                                              'l','m','n','o','p','q','r','s','t','u',\n",
    "                                                              'v','w','x','y','z','aa','bb'])\n",
    "\n",
    "# store\n",
    "store['ustrain_chassis'] = ustrain_chassis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import SBE3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get list of data files\n",
    "filenames = glob.glob('/home/tjc/research/flowmow/**/sbe3/*.DAT', recursive=True)\n",
    "filenames.sort()\n",
    "\n",
    "# loop through all files and append data to list\n",
    "sbe3_list = []\n",
    "for filename in filenames:\n",
    "    dive_number = np.int64(filename.split('/')[5][-3:])\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'RAW' in line.strip()[0:3]:\n",
    "                if len(line) == 50: # good lines from this instrument are length 50\n",
    "                    timestamp, epoch = get_timestamp(line)\n",
    "                    counts = list(map(np.int64, line.strip().split(' ')[4:6]))\n",
    "                    sbe3_list.append([timestamp, epoch, dive_number, counts[0], counts[1]])\n",
    "\n",
    "# convert to dataframe\n",
    "sbe3 = pd.DataFrame(sbe3_list, columns=['timestamp','epoch','dive_number','counts_0','counts_1'])\n",
    "\n",
    "# store\n",
    "store['sbe3'] = sbe3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Close store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.flush(fsync=True)\n",
    "store.close()\n",
    "del store"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
