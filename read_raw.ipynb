{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read_raw\n",
    "\n",
    "This notebook reads the raw FlowMow2 Sentry data into Pandas dataframes and saves them to an HDF5 file using HDFStore. This notebook does not do any processing on the data aside from assigning timestamps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timestamp extractor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp(line):\n",
    "    timestamp = dt.datetime.strptime(' '.join(line.strip().split(' ')[1:3]), '%Y/%m/%d %H:%M:%S.%f')\n",
    "    epoch = timestamp.replace(tzinfo=dt.timezone.utc).timestamp() # 'epoch' is unix time\n",
    "    return timestamp, epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import SCC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get list of data files\n",
    "filenames = glob.glob('/home/tjc/research/flowmow/**/*.scc', recursive=True)\n",
    "filenames.sort()\n",
    "\n",
    "# loop through all files and append data to list\n",
    "scc_list = []\n",
    "for filename in filenames:\n",
    "    dive_number = int(filename.split('/')[5][-3:])\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'SCC' in line.strip()[0:3]:\n",
    "                timestamp, epoch = get_timestamp(line)\n",
    "                scc_list.append([timestamp, epoch, dive_number] + list(map(np.float64, line.strip().split(' ')[3:])))\n",
    "\n",
    "# convert to dataframe\n",
    "scc = pd.DataFrame(scc_list, columns=['timestamp','epoch','dive_number','lat','lon','depth','pressure',\n",
    "                                      'heading','magx','magy','magz','obs','eh','aux1','aux2','T1','C1',\n",
    "                                      'T2','C2','S1','S2','ss1','depth_d','height','D1','D2'])\n",
    "\n",
    "# store to HDF5 store\n",
    "store = pd.HDFStore('flowmow.h5')\n",
    "store['scc'] = scc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Paros pressure sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get list of data files\n",
    "filenames = glob.glob('/home/tjc/research/flowmow/**/parosKinsey/*.DAT', recursive=True)\n",
    "filenames.sort()\n",
    "\n",
    "# loop through all files and append data to list\n",
    "paros_list = []\n",
    "i = 0\n",
    "for filename in filenames:\n",
    "    dive_number = int(filename.split('/')[5][-3:])\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'RAW' in line.strip()[0:3]:\n",
    "                if 'P2=' in line.strip():\n",
    "                    timestamp, epoch = get_timestamp(line)\n",
    "                    a = line.strip().split(' ')[3].split(',')[0].split('=')[1]\n",
    "                    b = line.strip().split(' ')[3].split(',')[1]\n",
    "                    paros_list.append([timestamp, epoch, dive_number, a, b])\n",
    "                    i = i+1\n",
    "\n",
    "# convert to dataframe\n",
    "paros = pd.DataFrame(paros_list, columns=['timestamp', 'epoch', 'dive_number', 'a', 'b'])\n",
    "\n",
    "# store\n",
    "store['paros'] = paros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import stinger GX3-25 microstrain IMU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get list of data files\n",
    "filenames = glob.glob('/home/tjc/research/flowmow/**/ustrainAdv/*.DAT', recursive=True)\n",
    "filenames.sort()\n",
    "\n",
    "# loop through all files and append data to list\n",
    "ustrain_adv_list = []\n",
    "for filename in filenames:\n",
    "    dive_number = int(filename.split('/')[5][-3:])\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'MSA3' in line.strip()[0:4]:\n",
    "                timestamp, epoch = get_timestamp(line)\n",
    "                ustrain_adv_list.append([timestamp, epoch, dive_number] + list(map(np.float64, line.strip().split(' ')[3:-1])))\n",
    "\n",
    "# convert to dataframe\n",
    "ustrain_adv = pd.DataFrame(ustrain_adv_list, columns=['timestamp','epoch','dive_number','a','b','c','d','e','f',\n",
    "                                                      'g','h','i','j','k','l','m','n','o','p','q',\n",
    "                                                      'r','s','t','u','v','w','x','y','z','aa','bb'])\n",
    "# store\n",
    "store['ustrain_adv'] = ustrain_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import chassis GX3-25 microstrain IMU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get list of data files\n",
    "filenames = glob.glob('/home/tjc/research/flowmow/**/ustrain-chassis/*.DAT', recursive=True)\n",
    "filenames.sort()\n",
    "\n",
    "# loop through all files and append data to list\n",
    "ustrain_chassis_list = []\n",
    "for filename in filenames:\n",
    "    dive_number = int(filename.split('/')[5][-3:])\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'MSA3' in line.strip()[0:4]:\n",
    "                timestamp, epoch = get_timestamp(line)\n",
    "                ustrain_chassis_list.append([timestamp, epoch, dive_number] + list(map(np.float64, line.strip().split(' ')[3:-1])))\n",
    "\n",
    "# convert to dataframe\n",
    "ustrain_chassis = pd.DataFrame(ustrain_chassis_list, columns=['timestamp','epoch','dive_number','a','b','c','d','e','f',\n",
    "                                                      'g','h','i','j','k','l','m','n','o','p','q',\n",
    "                                                      'r','s','t','u','v','w','x','y','z','aa','bb'])\n",
    "\n",
    "# store\n",
    "store['ustrain_chassis'] = ustrain_chassis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import SBE3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# get list of data files\n",
    "filenames = glob.glob('/home/tjc/research/flowmow/**/sbe3/*.DAT', recursive=True)\n",
    "filenames.sort()\n",
    "\n",
    "# loop through all files and append data to list\n",
    "sbe3_list = []\n",
    "for filename in filenames:\n",
    "    dive_number = int(filename.split('/')[5][-3:])\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'RAW' in line.strip()[0:3]:\n",
    "                timestamp, epoch = get_timestamp(line)\n",
    "                sbe3_list.append([timestamp, epoch, dive_number] + list(map(np.uint32, line.strip().split(' ')[4:6])))\n",
    "\n",
    "# convert to dataframe\n",
    "sbe3 = pd.DataFrame(sbe3_list, columns=['timestamp','epoch','dive_number','a','b'])\n",
    "\n",
    "# store\n",
    "store['sbe3'] = sbe3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Close store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.flush(fsync=True)\n",
    "store.close()\n",
    "del store"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
